{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTCOMES_PATH = \"../data/symptom_outcomes.csv\"\n",
    "PATH_TO_AUDIO = \"/vol0/psych_audio/raw-audio/\"\n",
    "MAX_ID_LEN = 6  # Some of the ID's in the pandas dataframe are cast to integers which makes them nonstandard lengths\n",
    "OUT_PATH = \"../results/scotty_phq9_diffs_with_paths.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are coded numerically in the dataset, \n",
    "# but when you read them in with pandas they are initially \n",
    "# coded as strings\n",
    "NUMERIC_COLS = [\n",
    "    'No_Interest_ses',\n",
    "    'Feeling_down_ses',\n",
    "    'Sleep_ses',\n",
    "    'Tired_ses',\n",
    "    'Appetite_ses',\n",
    "    'Bad_self_ses',\n",
    "    'Concentrating_ses',\n",
    "    'Move_slow_ses',\n",
    "    'Dead_ses',\n",
    "    'Work_ses',\n",
    "    'Large_amt_ses',\n",
    "    'OBE_LOC_ses',\n",
    "    'Vomit_ses',\n",
    "    'Laxative_ses',\n",
    "    'Restrict_ses',\n",
    "    'Shp_wt_judge_ses',\n",
    "    'Schoolwork_ses',\n",
    "    'Relationships_ses',\n",
    "    'Shp_Wt_Bad_self_ses',\n",
    "    'PHQ9_total_ses',\n",
    "]\n",
    "\n",
    "# Create some simple converter functions to turn strings\n",
    "# into integer format and date strings into date format\n",
    "STR_TO_INT = lambda val: None if val == \" \" else int(val)\n",
    "STR_TO_DATE = lambda val: None if val == \" \" else datetime.datetime.strptime(val, \"%m/%d/%Y\")\n",
    "\n",
    "# Build a dictionary of functions where the key\n",
    "# is the column name and the value is the converter \n",
    "# function which will convert from strings to the \n",
    "# appropriate type\n",
    "CONVERTERS = {\n",
    "    'Date_ses': STR_TO_DATE, \n",
    "    'Date_ses1': STR_TO_DATE,\n",
    "    'Age_ses1': STR_TO_INT, \n",
    "    'Weight_ses1': STR_TO_INT,\n",
    "    'Height_ses1': STR_TO_INT,\n",
    "    'Gender_ses1': STR_TO_INT,\n",
    "}\n",
    "\n",
    "# Systematically go through the numeric columns and convert\n",
    "# them from string format to integer format using the \n",
    "# lambda functions we created above\n",
    "for numeric_col in NUMERIC_COLS:\n",
    "    CONVERTERS[numeric_col] = STR_TO_INT\n",
    "    CONVERTERS[numeric_col + \"1\"] = STR_TO_INT  # Process all first-session-specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash(row):\n",
    "    \"\"\" Converts pandas df row into 256-bit hash\n",
    "    \n",
    "    Takes in a row from a pandas dataframe (presumably from the metadata\n",
    "    file) and, using the 'ID_number' and 'Date_ses' columns, provides a \n",
    "    unique 256-bit hash for that patient's session. This unique hash will \n",
    "    eventually become the audio path filename. \n",
    "    \"\"\"\n",
    "    hash_input = str(row['ID_number']) + ':' + row['Date_ses'].strftime(\"%Y-%m-%d\")\n",
    "    hash_obj = hashlib.sha256(hash_input.encode('utf-8'))\n",
    "    hex_dig = hash_obj.hexdigest()\n",
    "    return hex_dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_outcomes(outcomes_path=OUTCOMES_PATH, converters=CONVERTERS):\n",
    "    \"\"\" Read in and clean raw outcomes data file\n",
    "    \n",
    "    The data file given to us by the authors compiling the original\n",
    "    study has some extraneous columns and an overall format that is\n",
    "    not necessarily conducive to the types of analyses we wish to run.\n",
    "    This function therefore reads in that original data file into a \n",
    "    pandas dataframe and does some minor preprocessing, including:\n",
    "        (1) Extracting the site, therapist, and patient ID's from the\n",
    "            session recording ID\n",
    "        (2) Collapsing special _ses1 columns (indicating that the session\n",
    "            is the first for the patient) into the non-first-session columns\n",
    "            of the same name\n",
    "        (3) Dropping sessions that have no date (these are not useful to us)\n",
    "        (4) Sorting the entries by (first) ID_number and (second) session date\n",
    "        (5) Converting numeric columns to numeric values appropriately\n",
    "    \"\"\"\n",
    "    outcomes_df = pd.read_csv(outcomes_path, converters=converters)\n",
    "    \n",
    "    # Extracts the site, therapist, and patient ID numbers\n",
    "    # The first 2 digits are of the ID corerspond to the site\n",
    "    # The next 2 digits correspond to the therapist\n",
    "    # The last 2 digits correspond to the patient\n",
    "    outcomes_df['Site_ID_number'] = outcomes_df['ID_number'].floordiv(10000)\n",
    "    outcomes_df['Therapist_ID_number'] = outcomes_df['ID_number'].floordiv(100)\n",
    "    outcomes_df['Patient_ID_number'] = outcomes_df['ID_number']\n",
    "    \n",
    "    # Sets first session attributes (Ex. sets Date_ses for rows the first session, for which only Date_ses1 is set)\n",
    "    outcomes_df.Date_ses.fillna(outcomes_df.Date_ses1, inplace=True)\n",
    "    \n",
    "    print(\"Subject {} has no date. Dropping...\".format(outcomes_df.loc[outcomes_df['Date_ses'].isnull(), 'ID_number']))\n",
    "    outcomes_df = outcomes_df.loc[~outcomes_df['Date_ses'].isnull(), :]\n",
    "    \n",
    "    # Fill in the columns that don't end in 1 with information from first session\n",
    "    # using the corresponding column names that do end in 1\n",
    "    for numeric_col in NUMERIC_COLS:\n",
    "        outcomes_df[numeric_col].fillna(outcomes_df[numeric_col + \"1\"], inplace=True)\n",
    "        \n",
    "    # Sorts values by date (after ID)\n",
    "    outcomes_df.sort_values(['ID_number', 'Date_ses'], inplace=True)\n",
    "    outcomes_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Gets rid of unnecessary first session columns\n",
    "    unnecessary_cols = [numeric_col + \"1\" for numeric_col in NUMERIC_COLS] + ['Date_ses1', 'First_ses', 'is_first_session']\n",
    "    outcomes_df.drop(unnecessary_cols, axis=1, inplace=True)\n",
    "\n",
    "    return outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_filenames(meta_df, audio_path=PATH_TO_AUDIO):\n",
    "    \"\"\" Adds audio_path and Num_sess columns to metadata file\n",
    "    \n",
    "    Walks through the directory tree given by audio_path argument and, \n",
    "    for each file in the directory, extracts the date of the recording,\n",
    "    the patient ID, and the session number (note that, because there are \n",
    "    gaps, it may be the case that the serial ordering in the metadata file\n",
    "    does not correspond to a serial ordering of session numbers - therefore\n",
    "    this information must be extracted from the original audio filename \n",
    "    itself). By matching on the patient ID and date, an 'audio_path' and\n",
    "    'Num_sess' entry are thus added to the metadata dataframe given by meta_df \n",
    "    for each row entry corresponding to an audio file discovered in 'audio_path'.\n",
    "    \"\"\"\n",
    "    for root, subFolders, files in os.walk(audio_path):\n",
    "        for filename in files:\n",
    "            filetype = filename.split(\".\")[-1]\n",
    "            assert filetype in {\"mp3\", \"MP3\", \"wav\", \"WAV\", \"wma\", \"WMA\"}\n",
    "            name = filename[:-4]\n",
    "            name_split = name.split(\"_\")\n",
    "            sess_num = name_split[0][1]  # Sess number is 2nd char of first token in filename (1 is 1st session)\n",
    "            subj_id = name_split[1]  # Grab the second token in the filename, which should correspond to subj_id\n",
    "            part_str = \"\"\n",
    "            part_num = -1  # If an audio file is split into two parts, we'll mark this flag to be the part number.\n",
    "                           # Otherwise, it will just remain -1 and no parts handling will be performed\n",
    "            \n",
    "            # Handle case where the last 5 chars are \"Part1\" or \"Part2\"\n",
    "            if name[-5:-1] == \"Part\" or name[-5:-1] == \"part\":\n",
    "                part_str = name[-5:]\n",
    "                part_num = [int(x) for x in part_str if x.isdigit()]\n",
    "                if len(part_num) > 1:  # If for whatever reason there's more than one digit...\n",
    "                    raise ValueError(\"Invalid part number: {}\".format(name))\n",
    "                else:\n",
    "                    part_num = part_num[0]\n",
    "            \n",
    "            # Handle the case where the last 3 chars are \"pt1\" or \"pt2\"\n",
    "            if name[-3:-1] == \"Pt\" or name[-3:-1] == \"pt\":\n",
    "                part_str = name[-3:]\n",
    "                part_num = [int(x) for x in part_str if x.isdigit()]\n",
    "                if len(part_num) > 1:  # If for whatever reason there's more than one digit\n",
    "                    raise ValueError(\"Invalid part number: {}\".format(name))\n",
    "                else:\n",
    "                    part_num = part_num[0]\n",
    "            \n",
    "            # Handling abnormal case 1: S2_Part 1_470101_P1_10.20.15.MP3\n",
    "            try:\n",
    "                int(subj_id)  # Check to see if the second token can be cast as a string\n",
    "            except:  # If not, then assume the second token is the \"part\" e.g. part 1 vs. part 2\n",
    "                print(\"\\nEncountered nonstandard formatting: \")\n",
    "                print(filename)\n",
    "                try:\n",
    "                    subj_id = name_split[2]\n",
    "                    print(\"Skipping second token. Using third token as subject ID: {}\\n\".format(subj_id))\n",
    "                    int(subj_id)\n",
    "                    part_str = name_split[1]\n",
    "                    part_num = [int(x) for x in part_str if x.isdigit()]\n",
    "                    if len(part_num) > 1:\n",
    "                        raise ValueError(\"Invalid part number: {}\".format(name))\n",
    "                    else:\n",
    "                        part_num = part_num[0]\n",
    "                except:\n",
    "                    raise ValueError(\"Failed to parse nonstandard formatting: {}\".format(filename))\n",
    "            \n",
    "            # Extract the date from \n",
    "            date_match = re.search(\"(P[0-9].|\\s|_)([0-9]{1,2}\\.[0-9]{1,2}\\.[0-9]{2,4})\", filename)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(2)  # Find the substring corresponding to the date in the filename\n",
    "                date_str_split = date_str.split('.')  # Split by period to extract [month, day, year]\n",
    "                if len(date_str_split[2]) > 2:  # If the year is '2014' rather than '14', change it to '14'\n",
    "                    date_str_split[2] = date_str_split[2].replace('20', '')\n",
    "                date_str_standardized = '.'.join(date_str_split)  # Join everything back together\n",
    "                date = datetime.datetime.strptime(date_str_standardized, \"%m.%d.%y\")  # Turn it into a datetime obj.\n",
    "                date = date.strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                raise AttributeError(\"No date found for {}!\".format(filename))\n",
    "            \n",
    "            path = root + '/' + filename\n",
    "            # print(\"ID_number = {}; Date_ses = {}\".format(subj_id, date))\n",
    "            # print(\"Random date string: {}\".format(meta_df.loc[1, 'Date_ses']))\n",
    "            subj_sess_slice = meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & (meta_df['Date_ses'] == date)]\n",
    "            \n",
    "            if len(subj_sess_slice) == 0:\n",
    "                print(\"WARNING: Audiofile {} has no metadata. Adding placeholder row to metadata...\".format(filename))\n",
    "                # print(\"len before adding row = {}\".format(len(meta_df)))\n",
    "                new_row = pd.DataFrame(columns=meta_df.columns)\n",
    "                new_row.loc[0, 'ID_number'] = int(subj_id)\n",
    "                new_row.loc[0, 'Patient_ID_number'] = int(subj_id)\n",
    "                new_row.loc[0, 'Num_sess'] = sess_num\n",
    "                new_row.loc[0, 'Date_ses'] = date\n",
    "                new_row.loc[0, 'audio_path'] = path\n",
    "                for c in new_row.columns:\n",
    "                    if not pd.isnull(new_row.loc[0, c]):\n",
    "                        new_row[c] = new_row[c].astype(meta_df.dtypes[c])\n",
    "                meta_df = meta_df.append(new_row)\n",
    "                # print(\"len after adding row = {}\".format(len(meta_df)))\n",
    "                # print(\"\\tExtracted information from audiofile is:\")\n",
    "                # print(\"\\t\\tsubj_id = {}\\n\\t\\tsess_num={}\\n\\t\\tdate={}\".format(subj_id, sess_num, date))\n",
    "                continue\n",
    "\n",
    "            if part_num == -1:  # Nothing special, just a single audio file\n",
    "                meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                            (meta_df['Date_ses'] == date), \n",
    "                            'audio_path'] = path\n",
    "            \n",
    "            elif part_num == 1:  # Corresponds to a two-part audio file where we're considering the first part\n",
    "                current_stored_path =  meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                                   (meta_df['Date_ses'] == date), \n",
    "                                                   'audio_path']\n",
    "                if pd.isnull(current_stored_path.item()):\n",
    "                    meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                (meta_df['Date_ses'] == date), \n",
    "                                'audio_path'] = path\n",
    "                else:\n",
    "                    meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                (meta_df['Date_ses'] == date), \n",
    "                                'audio_path'] = path + \";\" + current_stored_path\n",
    "            \n",
    "            elif part_num == 2:  # Corresponds to a two-part audio file where we're considering the second part\n",
    "                current_stored_path =  meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                                   (meta_df['Date_ses'] == date), \n",
    "                                                   'audio_path']\n",
    "                if pd.isnull(current_stored_path.item()):\n",
    "                    meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                (meta_df['Date_ses'] == date), \n",
    "                                'audio_path'] = path\n",
    "                else:\n",
    "                    meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                (meta_df['Date_ses'] == date), \n",
    "                                'audio_path'] = current_stored_path + \";\" + path\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(\"ERROR: Our code can't handle more than 2-part audio files\")\n",
    "            \n",
    "            meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                                (meta_df['Date_ses'] == date), \n",
    "                                'Num_sess'] = sess_num\n",
    "            \n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_col_diffs_per_sess(diff_df, col_name='PHQ9_total_ses'):\n",
    "    \"\"\"Calculates session-to-session differences for a column\n",
    "    \n",
    "        Finds the session-to-session differences for a given column,\n",
    "        grouping by patient ID number and date. Thus, if we passed in \n",
    "        'PHQ9' as the diff column, then the entry under 'PHQ9_diff' for\n",
    "        each patient's first session will be that patient's second session\n",
    "        PHQ-9 score minus their first session PHQ-9 score.\n",
    "        \n",
    "        Args:\n",
    "            diff_df: pandas dataframe with columns\n",
    "                'ID_number' corresponding to the patient ID number\n",
    "                'Date_ses' corresponding the date of the session\n",
    "            col_name: The name of the column for which the user\n",
    "                would like to calculate session-to-session differences.\n",
    "                Default is 'PHQ9_total_ses'\n",
    "    \"\"\"\n",
    "    # First create a MultiIndex with outer index as the ID number\n",
    "    # and the inner index as the date\n",
    "    diff_df = diff_df.set_index(['ID_number', 'Date_ses'])\n",
    "    diff_df[col_name + '_diff'] = np.nan\n",
    "    idx = pd.IndexSlice  # Just some syntactic sugar for MultiIndex slicing\n",
    "    for id_num in diff_df.index.levels[0]:  # For each ID number...\n",
    "        # Go over all the dates and collect the differences between the previous\n",
    "        # session and the subsequent session. We use the option \n",
    "        diff_df.loc[idx[id_num, :], col_name + '_diff'] = -diff_df.loc[idx[id_num, :], \n",
    "                                                                      col_name].diff(periods=-1)\n",
    "    diff_df.reset_index(inplace=True)\n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = preprocess_outcomes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = extract_audio_filenames(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.sort_values(by=['ID_number', 'Date_ses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = calculate_col_diffs_per_sess(meta_df, col_name='PHQ9_total_ses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['hash'] = meta_df.apply(make_hash, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.to_csv(OUT_PATH, sep='\\t', float_format='%.2f', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Train, Dev, and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXT_PATH = \"/vol0/psych_audio/gold-transcripts/gold-final\"\n",
    "METADATA_PATH = \"/vol0/psych_audio/scotty/results/scotty_phq9_diffs_with_paths.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_subj_ids(test_files_path=TEST_TEXT_PATH):\n",
    "    \"\"\"Extract set of test subject IDs.\n",
    "    \n",
    "    Use session ID's that appear in the the gold-standard transcripts\n",
    "    as the list of test set session ID's. \n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        The set of test subject IDs (set of strings, all of length MAX_ID_LEN)\n",
    "    \"\"\"\n",
    "    test_subj_ids = set()\n",
    "    for filename in os.listdir(test_files_path):\n",
    "        name_split = filename.split('_')\n",
    "        if name_split[0] == \".DS\":  # If there's an arbitrary .DS file, just skip it\n",
    "            continue\n",
    "        else:  # Otherwise extract the subject ID from the filename and add it to the test set\n",
    "            test_subj_ids.add(name_split[1])\n",
    "    test_subj_ids = {x.rjust(MAX_ID_LEN, '0') for x in test_subj_ids}  # Left-pad with zeros\n",
    "    return test_subj_ids\n",
    "\n",
    "def extract_train_dev_combined_subj_ids(test_subj_ids, metadata_path=METADATA_PATH):\n",
    "    \"\"\"Extract union of train and dev IDs \n",
    "    \n",
    "    Use the metadata file to get the list of all ID's for which we have \n",
    "    PHQ-9 data, then remove from that list of ID's all the ones that are \n",
    "    already in the dev set, and keep the rest as the train/dev sets.\n",
    "    \n",
    "    Args:\n",
    "        test_subj_ids: (set of strings) the set of IDs associated with the test set\n",
    "    \n",
    "    Returns:\n",
    "        The set of train+dev subject IDs (set of strings, all of length MAX_ID_LEN)\n",
    "    \"\"\"\n",
    "    meta_df = pd.read_csv(metadata_path, delimiter=\"\\t\")\n",
    "    all_metadata_ids = np.unique(meta_df['ID_number'])  # Get the unique ID's from the metadata\n",
    "    all_metadata_ids = {str(x) for x in all_metadata_ids}\n",
    "    all_metadata_ids = {x.rjust(MAX_ID_LEN, '0') for x in all_metadata_ids}  # Left-pad with zeros\n",
    "    train_dev_ids = all_metadata_ids - test_subj_ids\n",
    "    return train_dev_ids, all_metadata_ids\n",
    "\n",
    "def extract_train_dev_split_subj_ids(train_dev_ids, n_dev=30, seed=42):\n",
    "    \"\"\"Extract the (split) set of train and dev IDs\n",
    "    \n",
    "    Randomly divide the IDs in the train_dev_ids to be in either \n",
    "    train set or dev set according such that there are n_dev IDs \n",
    "    in the dev set. The returned sets are disjoint, but their union\n",
    "    is the original train_dev_ids set passed as an argument.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    dev_subj_ids = set(random.sample(train_dev_ids, n_dev))\n",
    "    train_subj_ids = train_dev_ids - dev_subj_ids\n",
    "    return train_subj_ids, dev_subj_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_PATH, delimiter=\"\\t\")\n",
    "path_to_hash_map = {df.loc[i, 'audio_path']:df.loc[i, 'hash'] for i in df.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subj_ids = extract_test_subj_ids(TEST_TEXT_PATH)\n",
    "train_dev_ids, all_metadata_ids = extract_train_dev_combined_subj_ids(test_subj_ids)\n",
    "train_subj_ids, dev_subj_ids = extract_train_dev_split_subj_ids(train_dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracted {} train ID's\".format(len(train_subj_ids)))\n",
    "print(\"Extracted {} dev ID's\".format(len(dev_subj_ids)))\n",
    "print(\"Extracted {} test ID's\".format(len(test_subj_ids)))\n",
    "print(\"Extracted {} total ID's\".format(len(train_subj_ids.union(dev_subj_ids).union(test_subj_ids))))\n",
    "print(\"There are {} unique ID's in the metadata table\".format(len(all_metadata_ids)))\n",
    "assert len(train_subj_ids.intersection(dev_subj_ids)) == 0\n",
    "assert len(train_subj_ids.intersection(test_subj_ids)) == 0\n",
    "assert len(dev_subj_ids.intersection(test_subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_ids = {int(x) for x in train_subj_ids}\n",
    "dev_subj_ids = {int(x) for x in dev_subj_ids}\n",
    "test_subj_ids = {int(x) for x in test_subj_ids}\n",
    "\n",
    "df['train'] = df['ID_number'].isin(train_subj_ids)\n",
    "df['dev'] = df['ID_number'].isin(dev_subj_ids)\n",
    "df['test'] = df['ID_number'].isin(test_subj_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_PATH, delimiter=\"\\t\")\n",
    "df = df.dropna(subset=['PHQ9_total_ses'])  # TODO: Do we want to drop all sessions without PHQ9 total?\n",
    "df = df.dropna(subset=['Num_sess'])  # TODO: Do we want to drop all sessions without the session number?\n",
    "df['Num_sess'] = df['Num_sess'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding text to the dataframe for downstream ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DEV_TEXT_PATH = \"/vol0/psych_audio/bootcamp-2018/Data/Transcriptions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_df(meta_df):\n",
    "    \"\"\"Create the text field where the transcript words go.\n",
    "    \"\"\"\n",
    "    \n",
    "    for root, subFolders, files in os.walk(TRAIN_DEV_TEXT_PATH):\n",
    "        for filename in files:\n",
    "            filetype = filename.split(\".\")[-1]\n",
    "            name = filename[:-4]\n",
    "            name_split = name.split(\"_\")\n",
    "            sess_num = name_split[0][1]  # Sess number is 2nd char of first token in filename (1 is 1st session)\n",
    "            subj_id = name_split[1]  # Grab the second token in the filename, which should correspond to subj_id\n",
    "            part_str = \"\"\n",
    "            part_num = -1  # If an audio file is split into two parts, we'll mark this flag to be the part number.\n",
    "                           # Otherwise, it will just remain -1 and no parts handling will be performed\n",
    "            \n",
    "            path = root + '/' + filename\n",
    "            \n",
    "            # Handling abnormal case 1: S2_Part 1_470101_P1_10.20.15.MP3\n",
    "            try:\n",
    "                int(subj_id)  # Check to see if the second token can be cast as a string\n",
    "            except:  # If not, then assume the second token is the \"part\" e.g. part 1 vs. part 2\n",
    "                print(\"\\nEncountered nonstandard formatting: \")\n",
    "                print(filename)\n",
    "                try:\n",
    "                    subj_id = name_split[2]\n",
    "                    print(\"Skipping second token. Using third token as subject ID: {}\\n\".format(subj_id))\n",
    "                    int(subj_id)\n",
    "                    part_str = name_split[1]\n",
    "                    part_num = [int(x) for x in part_str if x.isdigit()]\n",
    "                    if len(part_num) > 1:\n",
    "                        raise ValueError(\"Invalid part number: {}\".format(name))\n",
    "                    else:\n",
    "                        part_num = part_num[0]\n",
    "                except:\n",
    "                    raise ValueError(\"Failed to parse nonstandard formatting: {}\".format(filename))\n",
    "            \n",
    "            with open(path) as f:\n",
    "                sess_words_list = []\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    entry = line.split(' ')\n",
    "                    if entry[0] == 'Word:':\n",
    "                        new_word = entry[1].lower()[:-1]\n",
    "                        sess_words_list.append(new_word)\n",
    "            sess_words_str = ' '.join(sess_words_list)\n",
    "            meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                        (meta_df['Num_sess'] == int(sess_num)),\n",
    "                        'text'] = sess_words_str\n",
    "            meta_df.loc[(meta_df['ID_number'] == int(subj_id)) & \\\n",
    "                        (meta_df['Num_sess'] == int(sess_num)),\n",
    "                        'text_path'] = path\n",
    "            \n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_text_to_df(df)\n",
    "df = df.dropna(subset=['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
